# ChÆ°Æ¡ng 5.4: Machine Learning cho VPA
## Dáº¡y MÃ¡y TÃ­nh Nháº­n Diá»‡n Pattern VPA

### ğŸ¯ Má»¥c TiÃªu ChÆ°Æ¡ng

Báº¡n Ä‘Ã£ biáº¿t cÃ¡ch nháº­n diá»‡n Stopping Volume, No Supply báº±ng máº¯t. NhÆ°ng náº¿u cÃ³ 100 cá»• phiáº¿u, liá»‡u báº¡n cÃ³ Ä‘á»§ thá»i gian kiá»ƒm tra háº¿t? ChÆ°Æ¡ng nÃ y sáº½ dáº¡y mÃ¡y tÃ­nh lÃ m viá»‡c Ä‘Ã³ cho báº¡n!

### ğŸ’¡ Ã TÆ°á»Ÿng Cá»‘t LÃµi

**"Dáº¡y mÃ¡y tÃ­nh nhÃ¬n charts nhÆ° má»™t chuyÃªn gia VPA"**

- Con ngÆ°á»i: NhÃ¬n 1 chart máº¥t 30 giÃ¢y
- MÃ¡y tÃ­nh: "NhÃ¬n" 1000 charts trong 1 giÃ¢y
- Káº¿t quáº£: TÃ¬m Ä‘Æ°á»£c cÆ¡ há»™i mÃ  con ngÆ°á»i cÃ³ thá»ƒ bá» lá»¡

### ğŸ¤– AI Sáº½ LÃ m GÃ¬ Cho Báº¡n?

1. **QuÃ©t toÃ n thá»‹ trÆ°á»ng** - Tá»± Ä‘á»™ng kiá»ƒm tra 800+ cá»• phiáº¿u VN
2. **PhÃ¡t hiá»‡n patterns** - TÃ¬m Stopping Volume, No Supply, Springs...
3. **Xáº¿p háº¡ng cÆ¡ há»™i** - ÄÆ°a ra top 10 cá»• phiáº¿u Ä‘Ã¡ng chÃº Ã½ nháº¥t
4. **Cáº£nh bÃ¡o realtime** - BÃ¡o ngay khi cÃ³ tÃ­n hiá»‡u máº¡nh

---

## ğŸ“š Pháº§n 1: CÆ¡ Báº£n - Dáº¡y AI Nháº­n Diá»‡n VPA

### A. Chuáº©n Bá»‹ Dá»¯ Liá»‡u Cho AI

```python
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

def tao_features_cho_ai(data_co_phieu, so_ngay_lookback=20):
    """
    Táº¡o features (Ä‘áº·c trÆ°ng) tá»« dá»¯ liá»‡u giÃ¡ Ä‘á»ƒ AI cÃ³ thá»ƒ há»c
    
    Features bao gá»“m:
    1. Volume features - Äáº·c trÆ°ng vá» khá»‘i lÆ°á»£ng  
    2. Price features - Äáº·c trÆ°ng vá» giÃ¡
    3. Technical indicators - CÃ¡c chá»‰ bÃ¡o ká»¹ thuáº­t
    """
    
    features_list = []
    
    for i in range(so_ngay_lookback, len(data_co_phieu)):
        window_data = data_co_phieu.iloc[i-so_ngay_lookback:i]
        current_day = data_co_phieu.iloc[i]
        
        # === VOLUME FEATURES ===
        volume_features = {
            # Khá»‘i lÆ°á»£ng tÆ°Æ¡ng Ä‘á»‘i
            'volume_ratio_5d': current_day['volume'] / window_data['volume'][-5:].mean(),
            'volume_ratio_10d': current_day['volume'] / window_data['volume'][-10:].mean(),
            'volume_ratio_20d': current_day['volume'] / window_data['volume'][-20:].mean(),
            
            # Z-score khá»‘i lÆ°á»£ng (Ä‘o má»©c Ä‘á»™ báº¥t thÆ°á»ng)
            'volume_zscore': (current_day['volume'] - window_data['volume'].mean()) / window_data['volume'].std(),
            
            # Xu hÆ°á»›ng khá»‘i lÆ°á»£ng
            'volume_trend_5d': (window_data['volume'][-5:].mean() / window_data['volume'][-10:-5].mean()) - 1,
            
            # Volume ranking (khá»‘i lÆ°á»£ng hÃ´m nay xáº¿p thá»© máº¥y trong 20 ngÃ y)
            'volume_percentile': (current_day['volume'] > window_data['volume']).sum() / len(window_data['volume'])
        }
        
        # === PRICE FEATURES ===
        price_features = {
            # Vá»‹ trÃ­ Ä‘Ã³ng cá»­a trong ngÃ y
            'close_position_in_day': (current_day['close'] - current_day['low']) / (current_day['high'] - current_day['low']) if current_day['high'] != current_day['low'] else 0.5,
            
            # Biáº¿n Ä‘á»™ng giÃ¡
            'daily_range': (current_day['high'] - current_day['low']) / current_day['close'],
            'price_change_1d': (current_day['close'] - window_data['close'].iloc[-2]) / window_data['close'].iloc[-2],
            'price_change_5d': (current_day['close'] - window_data['close'][-6]) / window_data['close'][-6],
            
            # Xu hÆ°á»›ng giÃ¡
            'price_trend_5d': (window_data['close'][-5:].mean() / window_data['close'][-10:-5].mean()) - 1,
            'price_trend_20d': (window_data['close'][-10:].mean() / window_data['close'][:10].mean()) - 1,
        }
        
        # === TECHNICAL INDICATORS ===
        tech_features = {
            # RSI Ä‘Æ¡n giáº£n
            'rsi_14': tinh_rsi_don_gian(window_data['close'], 14),
            
            # Moving averages
            'price_vs_ma5': current_day['close'] / window_data['close'][-5:].mean() - 1,
            'price_vs_ma10': current_day['close'] / window_data['close'][-10:].mean() - 1,
            'price_vs_ma20': current_day['close'] / window_data['close'].mean() - 1,
        }
        
        # === VPA-SPECIFIC FEATURES ===
        vpa_features = {
            # Stopping Volume indicators
            'stopping_volume_score': tinh_diem_tin_cay_stopping_volume_simple(window_data, current_day),
            
            # No Supply indicators
            'no_supply_likelihood': tinh_kha_nang_no_supply(window_data, current_day),
            
            # Volume-Price divergence
            'volume_price_correlation': tinh_tuong_quan_volume_price(window_data),
        }
        
        # Káº¿t há»£p táº¥t cáº£ features
        all_features = {**volume_features, **price_features, **tech_features, **vpa_features}
        all_features['date'] = current_day['date']
        all_features['symbol'] = current_day.get('symbol', 'UNKNOWN')
        
        features_list.append(all_features)
    
    return pd.DataFrame(features_list)

def tinh_rsi_don_gian(prices, period=14):
    """TÃ­nh RSI Ä‘Æ¡n giáº£n"""
    deltas = np.diff(prices)
    gains = np.where(deltas > 0, deltas, 0)
    losses = np.where(deltas < 0, -deltas, 0)
    
    avg_gain = np.mean(gains[-period:]) if len(gains) >= period else np.mean(gains)
    avg_loss = np.mean(losses[-period:]) if len(losses) >= period else np.mean(losses)
    
    if avg_loss == 0:
        return 100
    else:
        rs = avg_gain / avg_loss
        return 100 - (100 / (1 + rs))

def tinh_diem_tin_cay_stopping_volume_simple(window_data, current_day):
    """TÃ­nh Ä‘iá»ƒm Stopping Volume Ä‘Æ¡n giáº£n cho AI"""
    volume_ratio = current_day['volume'] / window_data['volume'][:-1].mean()
    close_position = (current_day['close'] - current_day['low']) / (current_day['high'] - current_day['low']) if current_day['high'] != current_day['low'] else 0.5
    
    volume_score = min(volume_ratio * 20, 60)  # Tá»‘i Ä‘a 60 Ä‘iá»ƒm
    close_score = close_position * 40          # Tá»‘i Ä‘a 40 Ä‘iá»ƒm
    
    return volume_score + close_score

def tinh_kha_nang_no_supply(window_data, current_day):
    """TÃ­nh kháº£ nÄƒng No Supply"""
    price_change = (current_day['close'] - current_day['open']) / current_day['open']
    volume_ratio = current_day['volume'] / window_data['volume'][:-1].mean()
    
    if price_change > 0.01 and volume_ratio < 0.8:  # GiÃ¡ tÄƒng > 1%, volume < 80% TB
        return min((price_change * 50) + ((0.8 - volume_ratio) * 100), 100)
    else:
        return 0

def tinh_tuong_quan_volume_price(window_data):
    """TÃ­nh tÆ°Æ¡ng quan giá»¯a volume vÃ  price"""
    prices = window_data['close'].values
    volumes = window_data['volume'].values
    
    if len(prices) < 2:
        return 0
    
    correlation = np.corrcoef(prices, volumes)[0, 1]
    return correlation if not np.isnan(correlation) else 0
```

### B. Táº¡o NhÃ£n (Labels) Cho AI Há»c

```python
def tao_nhan_cho_ai(data_co_phieu, features_df, so_ngay_forward=5, nguong_loi_nhuan=0.02):
    """
    Táº¡o nhÃ£n cho AI há»c:
    - 1: TÃ­n hiá»‡u Tá»T (giÃ¡ tÄƒng > 2% trong 5 ngÃ y)
    - 0: TÃ­n hiá»‡u Xáº¤U (giÃ¡ khÃ´ng tÄƒng hoáº·c giáº£m)
    """
    
    labels = []
    
    for i, row in features_df.iterrows():
        # TÃ¬m vá»‹ trÃ­ trong dá»¯ liá»‡u gá»‘c
        current_date = row['date']
        current_idx = data_co_phieu[data_co_phieu['date'] == current_date].index[0]
        
        # Kiá»ƒm tra cÃ³ Ä‘á»§ dá»¯ liá»‡u forward khÃ´ng
        if current_idx + so_ngay_forward < len(data_co_phieu):
            gia_hien_tai = data_co_phieu.iloc[current_idx]['close']
            gia_tuong_lai = data_co_phieu.iloc[current_idx + so_ngay_forward]['close']
            
            loi_nhuan = (gia_tuong_lai - gia_hien_tai) / gia_hien_tai
            
            # GÃ¡n nhÃ£n
            if loi_nhuan >= nguong_loi_nhuan:
                nhan = 1  # TÃ­n hiá»‡u Tá»T
            else:
                nhan = 0  # TÃ­n hiá»‡u Xáº¤U
        else:
            nhan = -1  # KhÃ´ng Ä‘á»§ dá»¯ liá»‡u
        
        labels.append({
            'date': current_date,
            'label': nhan,
            'forward_return': loi_nhuan if nhan != -1 else None
        })
    
    return pd.DataFrame(labels)

# Sá»­ dá»¥ng
vcb_data = pd.read_csv('market_data/VCB.csv')
vcb_features = tao_features_cho_ai(vcb_data)
vcb_labels = tao_nhan_cho_ai(vcb_data, vcb_features)

print("=== Dá»® LIá»†U ÄÃƒ CHUáº¨N Bá»Š CHO AI ===")
print(f"Sá»‘ máº«u features: {len(vcb_features)}")
print(f"Sá»‘ features: {len(vcb_features.columns) - 2}")  # Trá»« date vÃ  symbol
print(f"Tá»· lá»‡ tÃ­n hiá»‡u Tá»T: {(vcb_labels['label'] == 1).sum() / len(vcb_labels):.1%}")

# Hiá»ƒn thá»‹ má»™t vÃ i features
print(f"\nVÃ­ dá»¥ features:")
feature_columns = [col for col in vcb_features.columns if col not in ['date', 'symbol']]
print(vcb_features[feature_columns[:5]].head())
```

### C. Huáº¥n Luyá»‡n AI Model Äáº§u TiÃªn

```python
def huan_luyen_ai_vpa_co_ban(features_df, labels_df):
    """
    Huáº¥n luyá»‡n model AI Ä‘Æ¡n giáº£n Ä‘á»ƒ nháº­n diá»‡n VPA patterns
    """
    
    # Káº¿t há»£p features vÃ  labels
    merged_data = pd.merge(features_df, labels_df, on='date')
    
    # Chá»‰ láº¥y nhá»¯ng máº«u cÃ³ label há»£p lá»‡
    valid_data = merged_data[merged_data['label'] != -1].copy()
    
    # Chuáº©n bá»‹ X (features) vÃ  y (labels)
    feature_columns = [col for col in features_df.columns if col not in ['date', 'symbol']]
    X = valid_data[feature_columns].fillna(0)  # Äiá»n giÃ¡ trá»‹ 0 cho missing values
    y = valid_data['label']
    
    # Chia train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # Huáº¥n luyá»‡n model Random Forest
    model = RandomForestClassifier(
        n_estimators=100,  # 100 cÃ¢y quyáº¿t Ä‘á»‹nh
        max_depth=10,      # Äá»™ sÃ¢u tá»‘i Ä‘a
        random_state=42,
        class_weight='balanced'  # CÃ¢n báº±ng tá»· lá»‡ class
    )
    
    model.fit(X_train, y_train)
    
    # ÄÃ¡nh giÃ¡ model
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    # Feature importance
    feature_importance = pd.DataFrame({
        'feature': feature_columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("=== Káº¾T QUáº¢ HUáº¤N LUYá»†N AI ===")
    print(f"âœ… Äá»™ chÃ­nh xÃ¡c: {accuracy:.1%}")
    print(f"ğŸ“Š Sá»‘ máº«u training: {len(X_train)}")
    print(f"ğŸ§ª Sá»‘ máº«u testing: {len(X_test)}")
    
    print(f"\nğŸ”¥ Top 5 features quan trá»ng nháº¥t:")
    for i, row in feature_importance.head().iterrows():
        print(f"   {i+1}. {row['feature']}: {row['importance']:.3f}")
    
    # BÃ¡o cÃ¡o chi tiáº¿t
    print(f"\nğŸ“‹ BÃ¡o cÃ¡o chi tiáº¿t:")
    print(classification_report(y_test, y_pred, target_names=['TÃ­n hiá»‡u Xáº¤U', 'TÃ­n hiá»‡u Tá»T']))
    
    return model, feature_columns, feature_importance

# Huáº¥n luyá»‡n model
ai_model, feature_cols, importance_df = huan_luyen_ai_vpa_co_ban(vcb_features, vcb_labels)
```

---

## ğŸ“ˆ Pháº§n 2: Thá»±c HÃ nh - Sá»­ Dá»¥ng AI Äá»ƒ QuÃ©t Thá»‹ TrÆ°á»ng

### A. Táº¡o Scanner Tá»± Äá»™ng

```python
def ai_vpa_scanner(co_phieu_list, ai_model, feature_columns, top_n=10):
    """
    QuÃ©t toÃ n bá»™ danh sÃ¡ch cá»• phiáº¿u vÃ  tÃ¬m nhá»¯ng cÆ¡ há»™i tá»‘t nháº¥t
    """
    
    ket_qua_scan = []
    
    for symbol in co_phieu_list:
        try:
            # Táº£i dá»¯ liá»‡u cá»• phiáº¿u
            data = pd.read_csv(f'market_data/{symbol}.csv')
            
            # Táº¡o features cho ngÃ y gáº§n nháº¥t
            features = tao_features_cho_ai(data)
            if len(features) == 0:
                continue
                
            # Láº¥y features ngÃ y cuá»‘i cÃ¹ng
            latest_features = features.iloc[-1]
            X_latest = latest_features[feature_columns].values.reshape(1, -1)
            
            # Dá»± Ä‘oÃ¡n báº±ng AI
            prediction = ai_model.predict(X_latest)[0]
            prediction_proba = ai_model.predict_proba(X_latest)[0]
            
            # Náº¿u AI dá»± Ä‘oÃ¡n lÃ  tÃ­n hiá»‡u Tá»T
            if prediction == 1:
                ket_qua_scan.append({
                    'symbol': symbol,
                    'date': latest_features['date'],
                    'ai_prediction': 'Tá»T',
                    'confidence': prediction_proba[1] * 100,  # XÃ¡c suáº¥t lÃ  tÃ­n hiá»‡u tá»‘t
                    'current_price': data.iloc[-1]['close'],
                    'volume_ratio': latest_features.get('volume_ratio_20d', 0),
                    'stopping_volume_score': latest_features.get('stopping_volume_score', 0),
                    'no_supply_score': latest_features.get('no_supply_likelihood', 0),
                })
        
        except Exception as e:
            print(f"âš ï¸ Lá»—i khi xá»­ lÃ½ {symbol}: {e}")
            continue
    
    # Sáº¯p xáº¿p theo confidence giáº£m dáº§n
    ket_qua_scan.sort(key=lambda x: x['confidence'], reverse=True)
    
    print(f"=== AI VPA SCANNER RESULTS ===")
    print(f"ğŸ” ÄÃ£ quÃ©t: {len(co_phieu_list)} cá»• phiáº¿u")
    print(f"âœ… TÃ¬m tháº¥y: {len(ket_qua_scan)} cÆ¡ há»™i")
    
    print(f"\nğŸ† TOP {min(top_n, len(ket_qua_scan))} Cá»” PHIáº¾U ÄÃNG CHÃš Ã:")
    for i, result in enumerate(ket_qua_scan[:top_n]):
        print(f"\n   {i+1}. {result['symbol']} - Tin cáº­y: {result['confidence']:.1f}%")
        print(f"      ğŸ’° GiÃ¡ hiá»‡n táº¡i: {result['current_price']:,}Ä‘")
        print(f"      ğŸ“Š Volume ratio: {result['volume_ratio']:.1f}x")
        print(f"      âš¡ Stopping Volume: {result['stopping_volume_score']:.0f}/100")
        print(f"      ğŸ”¥ No Supply: {result['no_supply_score']:.0f}/100")
    
    return ket_qua_scan

# Danh sÃ¡ch cá»• phiáº¿u Ä‘á»ƒ quÃ©t (vÃ­ dá»¥)
co_phieu_vn30 = ['VCB', 'TCB', 'BID', 'VIC', 'VHM', 'HPG', 'VRE', 'MSN', 'SAB', 'CTG']

# Cháº¡y scanner
scan_results = ai_vpa_scanner(co_phieu_vn30, ai_model, feature_cols, top_n=5)
```

### B. Backtesting AI Model

```python
def backtest_ai_model(ai_model, feature_columns, test_data, so_ngay_giu=5):
    """
    Kiá»ƒm tra hiá»‡u quáº£ cá»§a AI model vá»›i dá»¯ liá»‡u lá»‹ch sá»­
    """
    
    giao_dich_list = []
    
    # Táº¡o features cho toÃ n bá»™ dá»¯ liá»‡u test
    features_test = tao_features_cho_ai(test_data)
    
    for i in range(len(features_test) - so_ngay_giu):
        current_features = features_test.iloc[i]
        X_current = current_features[feature_columns].values.reshape(1, -1)
        
        # AI dá»± Ä‘oÃ¡n
        prediction = ai_model.predict(X_current)[0]
        confidence = ai_model.predict_proba(X_current)[0][1]
        
        # Chá»‰ giao dá»‹ch khi AI tin tÆ°á»Ÿng > 70%
        if prediction == 1 and confidence > 0.7:
            # TÃ¬m giÃ¡ mua/bÃ¡n
            ngay_mua_idx = test_data[test_data['date'] == current_features['date']].index[0]
            ngay_ban_idx = ngay_mua_idx + so_ngay_giu
            
            if ngay_ban_idx < len(test_data):
                gia_mua = test_data.iloc[ngay_mua_idx]['close']
                gia_ban = test_data.iloc[ngay_ban_idx]['close']
                
                loi_nhuan = (gia_ban - gia_mua) / gia_mua
                
                giao_dich_list.append({
                    'ngay_mua': current_features['date'],
                    'gia_mua': gia_mua,
                    'ngay_ban': test_data.iloc[ngay_ban_idx]['date'],
                    'gia_ban': gia_ban,
                    'loi_nhuan': loi_nhuan,
                    'ai_confidence': confidence
                })
    
    # PhÃ¢n tÃ­ch káº¿t quáº£
    if giao_dich_list:
        loi_nhuan_list = [gd['loi_nhuan'] for gd in giao_dich_list]
        
        tong_gd = len(giao_dich_list)
        gd_thang = sum(1 for ln in loi_nhuan_list if ln > 0)
        ty_le_thang = gd_thang / tong_gd
        loi_nhuan_tb = np.mean(loi_nhuan_list)
        loi_nhuan_tong = sum(loi_nhuan_list)
        
        print("=== Káº¾T QUáº¢ BACKTEST AI MODEL ===")
        print(f"ğŸ¤– AI Model Performance:")
        print(f"   â€¢ Tá»•ng giao dá»‹ch: {tong_gd}")
        print(f"   â€¢ Giao dá»‹ch tháº¯ng: {gd_thang}")
        print(f"   â€¢ Tá»· lá»‡ tháº¯ng: {ty_le_thang:.1%}")
        print(f"   â€¢ Lá»£i nhuáº­n trung bÃ¬nh: {loi_nhuan_tb:.2%}")
        print(f"   â€¢ Tá»•ng lá»£i nhuáº­n: {loi_nhuan_tong:.2%}")
        
        # So sÃ¡nh vá»›i benchmark
        gia_dau = test_data.iloc[0]['close']
        gia_cuoi = test_data.iloc[-1]['close']
        benchmark_return = (gia_cuoi - gia_dau) / gia_dau
        
        print(f"\nğŸ“Š So sÃ¡nh vá»›i Buy & Hold:")
        print(f"   â€¢ Buy & Hold return: {benchmark_return:.2%}")
        print(f"   â€¢ AI Model return: {loi_nhuan_tong:.2%}")
        print(f"   â€¢ Outperformance: {loi_nhuan_tong - benchmark_return:.2%}")
        
        return {
            'total_trades': tong_gd,
            'win_rate': ty_le_thang,
            'avg_return': loi_nhuan_tb,
            'total_return': loi_nhuan_tong,
            'outperformance': loi_nhuan_tong - benchmark_return
        }
    
    return None

# Backtest AI model vá»›i dá»¯ liá»‡u VCB
backtest_results = backtest_ai_model(ai_model, feature_cols, vcb_data)
```

---

## ğŸ” Pháº§n 3: NÃ¢ng Cao - Deep Learning VPA

> ğŸ’¡ **LÆ°u Ã½**: Pháº§n nÃ y dÃ nh cho ngÆ°á»i muá»‘n tÃ¬m hiá»ƒu vá» Deep Learning. 
> Náº¿u báº¡n má»›i báº¯t Ä‘áº§u vá»›i AI, cÃ³ thá»ƒ **bá» qua** vÃ  quay láº¡i sau.

### A. CNN Äá»ƒ Nháº­n Diá»‡n CÃ¡c Máº«u Biá»ƒu Äá»“

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

def tao_hinh_anh_bieu_do_cho_cnn(du_lieu_co_phieu, kich_thuoc_cua_so=50, kich_thuoc_hinh=(64, 64)):
    """
    Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u OHLCV thÃ nh hÃ¬nh áº£nh Ä‘á»ƒ CNN cÃ³ thá»ƒ há»c
    """
    
    hinh_anh_bieu_do = []
    nhan = []
    
    for i in range(kich_thuoc_cua_so, len(du_lieu_co_phieu) - 5):  # -5 Ä‘á»ƒ cÃ³ lá»£i nhuáº­n tÆ°Æ¡ng lai
        # Láº¥y dá»¯ liá»‡u cá»­a sá»•
        du_lieu_cua_so = du_lieu_co_phieu.iloc[i-kich_thuoc_cua_so:i]
        
        # Táº¡o hÃ¬nh áº£nh biá»ƒu Ä‘á»“ (Ä‘Æ¡n giáº£n hÃ³a)
        hinh_anh_bieu_do_item = tao_ma_tran_hinh_nen(du_lieu_cua_so, kich_thuoc_hinh)
        
        # TÃ­nh lá»£i nhuáº­n tÆ°Æ¡ng lai Ä‘á»ƒ lÃ m nhÃ£n
        gia_hien_tai = du_lieu_co_phieu.iloc[i]['close']
        gia_tuong_lai = du_lieu_co_phieu.iloc[i+5]['close']
        loi_nhuan_tuong_lai = (gia_tuong_lai - gia_hien_tai) / gia_hien_tai
        
        # NhÃ£n nhá»‹ phÃ¢n: 1 náº¿u lá»£i nhuáº­n > 2%, 0 náº¿u khÃ´ng
        nhan_item = 1 if loi_nhuan_tuong_lai > 0.02 else 0
        
        hinh_anh_bieu_do.append(hinh_anh_bieu_do_item)
        nhan.append(nhan_item)
    
    return np.array(hinh_anh_bieu_do), np.array(nhan)

def tao_ma_tran_hinh_nen(du_lieu_cua_so, kich_thuoc_hinh):
    """
    Táº¡o ma tráº­n hÃ¬nh áº£nh tá»« dá»¯ liá»‡u candlestick
    """
    chieu_cao, chieu_rong = kich_thuoc_hinh
    ma_tran_hinh = np.zeros((chieu_cao, chieu_rong, 3))  # CÃ¡c kÃªnh RGB
    
    # Chuáº©n hÃ³a dá»¯ liá»‡u giÃ¡ vá» [0, 1]
    gia_min = du_lieu_cua_so[['open', 'high', 'low', 'close']].min().min()
    gia_max = du_lieu_cua_so[['open', 'high', 'low', 'close']].max().max()
    
    khoi_luong_min = du_lieu_cua_so['volume'].min()
    khoi_luong_max = du_lieu_cua_so['volume'].max()
    
    for i, (_, ngay) in enumerate(du_lieu_cua_so.iterrows()):
        if i >= chieu_rong:  # KhÃ´ng vÆ°á»£t quÃ¡ chiá»u rá»™ng cá»§a hÃ¬nh
            break
            
        # Chuáº©n hÃ³a giÃ¡
        mo_norm = (ngay['open'] - gia_min) / (gia_max - gia_min)
        cao_norm = (ngay['high'] - gia_min) / (gia_max - gia_min)
        thap_norm = (ngay['low'] - gia_min) / (gia_max - gia_min)
        dong_norm = (ngay['close'] - gia_min) / (gia_max - gia_min)
        khoi_luong_norm = (ngay['volume'] - khoi_luong_min) / (khoi_luong_max - khoi_luong_min)
        
        # Chuyá»ƒn vá» toáº¡ Ä‘á»™ pixel
        mo_y = int((1 - mo_norm) * (chieu_cao - 1))
        cao_y = int((1 - cao_norm) * (chieu_cao - 1))
        thap_y = int((1 - thap_norm) * (chieu_cao - 1))
        dong_y = int((1 - dong_norm) * (chieu_cao - 1))
        
        # Váº½ náº¿n
        # KÃªnh Ä‘á»: HÃ nh Ä‘á»™ng giÃ¡
        for y in range(min(cao_y, thap_y), max(cao_y, thap_y) + 1):
            ma_tran_hinh[y, i, 0] = 0.5  # ÄÆ°á»ng cao-tháº¥p
        
        # ThÃ¢n cá»§a náº¿n
        than_bat_dau = min(mo_y, dong_y)
        than_ket_thuc = max(mo_y, dong_y)
        
        if dong_norm > mo_norm:  # Náº¿n xanh
            ma_tran_hinh[than_bat_dau:than_ket_thuc+1, i, 1] = 1.0  # KÃªnh xanh
        else:  # Náº¿n Ä‘á»
            ma_tran_hinh[than_bat_dau:than_ket_thuc+1, i, 0] = 1.0  # KÃªnh Ä‘á»
        
        # KÃªnh xanh dÆ°Æ¡ng: Khá»‘i lÆ°á»£ng
        chieu_cao_khoi_luong = int(khoi_luong_norm * chieu_cao * 0.3)  # Cá»™t khá»‘i lÆ°á»£ng á»Ÿ dÆ°á»›i
        if chieu_cao_khoi_luong > 0:
            ma_tran_hinh[-chieu_cao_khoi_luong:, i, 2] = khoi_luong_norm
    
    return ma_tran_hinh

def tao_mo_hinh_cnn(hinh_dang_dau_vao):
    """
    Táº¡o mÃ´ hÃ¬nh CNN Ä‘á»ƒ nháº­n diá»‡n cÃ¡c máº«u VPA tá»« hÃ¬nh áº£nh biá»ƒu Ä‘á»“
    """
    
    mo_hinh = keras.Sequential([
        # CÃ¡c lá»›p tÃ­ch cháº­p
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=hinh_dang_dau_vao),
        layers.MaxPooling2D((2, 2)),
        
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        
        # CÃ¡c lá»›p dÃ y Ä‘áº·c
        layers.Flatten(),
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(1, activation='sigmoid')  # Binary classification
    ])
    
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    
    return model

# Chuáº©n bá»‹ dá»¯ liá»‡u vÃ  training CNN (vÃ­ dá»¥ conceptual)
# Trong thá»±c táº¿ cáº§n nhiá»u dá»¯ liá»‡u hÆ¡n
print("=== DEEP LEARNING VPA - CNN APPROACH ===")
print("ğŸ’¡ ÄÃ¢y lÃ  vÃ­ dá»¥ conceptual. Trong thá»±c táº¿ cáº§n:")
print("   â€¢ Nhiá»u dá»¯ liá»‡u hÆ¡n (1000+ cá»• phiáº¿u)")
print("   â€¢ GPU Ä‘á»ƒ training nhanh")
print("   â€¢ Fine-tuning nhiá»u parameters")
print("   â€¢ Data augmentation techniques")

# X_images, y_labels = tao_chart_images_cho_cnn(vcb_data)
# cnn_model = tao_cnn_model((64, 64, 3))
# cnn_model.fit(X_images, y_labels, epochs=50, validation_split=0.2)
```

---

> ğŸ”¥ **PHáº¦N NÃ‚NG CAO - CÃ“ THá»‚ Bá» QUA Náº¾U Má»šI Báº®T Äáº¦U**

<details>
<summary>ğŸ“‹ <strong>Advanced ML Pipeline - Production Ready System</strong></summary>

```python
class ProductionVPAMLSystem:
    def __init__(self):
        self.models = {
            'random_forest': None,
            'gradient_boosting': None,
            'neural_network': None,
            'ensemble': None
        }
        
        self.feature_pipeline = None
        self.scaler = None
        self.feature_selector = None
        
    def create_advanced_features(self, stock_data, lookback_period=50):
        """
        Táº¡o features nÃ¢ng cao cho production system
        """
        
        advanced_features = []
        
        for i in range(lookback_period, len(stock_data)):
            window_data = stock_data.iloc[i-lookback_period:i]
            current_day = stock_data.iloc[i]
            
            # Advanced Volume Features
            volume_features = self._extract_volume_features(window_data, current_day)
            
            # Advanced Price Action Features
            price_features = self._extract_price_action_features(window_data, current_day)
            
            # Market Microstructure Features
            microstructure_features = self._extract_microstructure_features(window_data, current_day)
            
            # Time Series Features
            time_series_features = self._extract_time_series_features(window_data)
            
            # VPA Specific Features
            vpa_features = self._extract_advanced_vpa_features(window_data, current_day)
            
            # Combine all features
            combined_features = {
                **volume_features,
                **price_features,
                **microstructure_features,
                **time_series_features,
                **vpa_features,
                'date': current_day['date'],
                'symbol': current_day.get('symbol', 'UNKNOWN')
            }
            
            advanced_features.append(combined_features)
        
        return pd.DataFrame(advanced_features)
    
    def _extract_volume_features(self, window_data, current_day):
        """Extract advanced volume-based features"""
        volumes = window_data['volume'].values
        prices = window_data['close'].values
        
        return {
            # Volume Distribution
            'volume_skewness': self._calculate_skewness(volumes),
            'volume_kurtosis': self._calculate_kurtosis(volumes),
            
            # Volume Persistence
            'volume_autocorr_1': self._calculate_autocorr(volumes, lag=1),
            'volume_autocorr_5': self._calculate_autocorr(volumes, lag=5),
            
            # Volume-Price Relationship
            'volume_price_elasticity': self._calculate_price_volume_elasticity(prices, volumes),
            'volume_weighted_price': np.average(prices, weights=volumes),
            
            # Volume Clustering
            'volume_cluster_strength': self._calculate_volume_clustering(volumes),
            
            # Abnormal Volume Detection
            'volume_anomaly_score': self._detect_volume_anomalies(volumes, current_day['volume']),
        }
    
    def _extract_price_action_features(self, window_data, current_day):
        """Extract advanced price action features"""
        opens = window_data['open'].values
        highs = window_data['high'].values
        lows = window_data['low'].values  
        closes = window_data['close'].values
        
        return {
            # Candlestick Patterns
            'doji_strength': self._calculate_doji_strength(opens, closes, highs, lows),
            'hammer_strength': self._calculate_hammer_strength(opens, closes, highs, lows),
            'engulfing_pattern': self._detect_engulfing_pattern(opens, closes),
            
            # Price Level Analysis
            'support_resistance_strength': self._calculate_sr_strength(highs, lows, closes),
            'breakout_strength': self._calculate_breakout_strength(highs, lows, closes),
            
            # Gap Analysis
            'gap_frequency': self._calculate_gap_frequency(opens, closes),
            'gap_fill_ratio': self._calculate_gap_fill_ratio(opens, closes, highs, lows),
            
            # Intraday Patterns
            'intraday_reversal_tendency': self._calculate_intraday_reversal(opens, closes, highs, lows),
            'closing_strength': self._calculate_closing_strength(opens, closes, highs, lows),
        }
    
    def _extract_microstructure_features(self, window_data, current_day):
        """Extract market microstructure features"""
        
        return {
            # Bid-Ask Spread Proxy (using high-low)
            'effective_spread_proxy': self._calculate_effective_spread_proxy(window_data),
            
            # Order Flow Imbalance Proxy
            'order_imbalance_proxy': self._calculate_order_imbalance_proxy(window_data),
            
            # Price Impact
            'price_impact_ratio': self._calculate_price_impact_ratio(window_data),
            
            # Liquidity Measures
            'liquidity_ratio': self._calculate_liquidity_ratio(window_data),
            'depth_proxy': self._calculate_market_depth_proxy(window_data),
        }
    
    def train_ensemble_model(self, X_train, y_train, X_val, y_val):
        """
        Train ensemble of multiple ML models
        """
        from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
        from sklearn.neural_network import MLPClassifier
        from sklearn.ensemble import VotingClassifier
        
        # Individual models
        rf_model = RandomForestClassifier(
            n_estimators=200,
            max_depth=15,
            min_samples_split=5,
            random_state=42
        )
        
        gb_model = GradientBoostingClassifier(
            n_estimators=200,
            learning_rate=0.1,
            max_depth=8,
            random_state=42
        )
        
        nn_model = MLPClassifier(
            hidden_layer_sizes=(100, 50, 25),
            activation='relu',
            solver='adam',
            alpha=0.001,
            batch_size='auto',
            learning_rate='constant',
            learning_rate_init=0.001,
            max_iter=500,
            random_state=42
        )
        
        # Ensemble model
        ensemble_model = VotingClassifier(
            estimators=[
                ('rf', rf_model),
                ('gb', gb_model),
                ('nn', nn_model)
            ],
            voting='soft'
        )
        
        # Train models
        print("Training Random Forest...")
        rf_model.fit(X_train, y_train)
        self.models['random_forest'] = rf_model
        
        print("Training Gradient Boosting...")
        gb_model.fit(X_train, y_train)
        self.models['gradient_boosting'] = gb_model
        
        print("Training Neural Network...")
        nn_model.fit(X_train, y_train)
        self.models['neural_network'] = nn_model
        
        print("Training Ensemble...")
        ensemble_model.fit(X_train, y_train)
        self.models['ensemble'] = ensemble_model
        
        # Evaluate models
        self._evaluate_models(X_val, y_val)
        
        return self.models
    
    def _evaluate_models(self, X_val, y_val):
        """Evaluate all trained models"""
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        
        print("\n=== MODEL EVALUATION ===")
        
        for model_name, model in self.models.items():
            if model is not None:
                y_pred = model.predict(X_val)
                
                accuracy = accuracy_score(y_val, y_pred)
                precision = precision_score(y_val, y_pred, average='weighted')
                recall = recall_score(y_val, y_pred, average='weighted')
                f1 = f1_score(y_val, y_pred, average='weighted')
                
                print(f"\n{model_name.upper()}:")
                print(f"   Accuracy:  {accuracy:.3f}")
                print(f"   Precision: {precision:.3f}")
                print(f"   Recall:    {recall:.3f}")
                print(f"   F1-Score:  {f1:.3f}")
    
    def real_time_prediction_pipeline(self, current_stock_data):
        """
        Real-time prediction pipeline for production use
        """
        
        # Feature extraction
        features = self.create_advanced_features(current_stock_data)
        if len(features) == 0:
            return None
        
        # Get latest features
        latest_features = features.iloc[-1]
        feature_columns = [col for col in features.columns if col not in ['date', 'symbol']]
        X_latest = latest_features[feature_columns].values.reshape(1, -1)
        
        # Feature preprocessing
        if self.scaler:
            X_latest = self.scaler.transform(X_latest)
        
        if self.feature_selector:
            X_latest = self.feature_selector.transform(X_latest)
        
        # Predictions from all models
        predictions = {}
        confidences = {}
        
        for model_name, model in self.models.items():
            if model is not None:
                pred = model.predict(X_latest)[0]
                pred_proba = model.predict_proba(X_latest)[0]
                
                predictions[model_name] = pred
                confidences[model_name] = pred_proba[1] if pred == 1 else pred_proba[0]
        
        # Ensemble decision
        ensemble_pred = predictions.get('ensemble', 0)
        ensemble_confidence = confidences.get('ensemble', 0.5)
        
        # Risk assessment
        risk_level = self._assess_prediction_risk(predictions, confidences)
        
        return {
            'symbol': latest_features.get('symbol', 'UNKNOWN'),
            'date': latest_features['date'],
            'predictions': predictions,
            'confidences': confidences,
            'ensemble_prediction': ensemble_pred,
            'ensemble_confidence': ensemble_confidence,
            'risk_level': risk_level,
            'recommendation': self._generate_recommendation(ensemble_pred, ensemble_confidence, risk_level)
        }
    
    def _assess_prediction_risk(self, predictions, confidences):
        """Assess the risk level of predictions"""
        
        # Check prediction consensus
        pred_values = list(predictions.values())
        consensus = len(set(pred_values)) == 1  # All models agree
        
        # Check confidence levels
        avg_confidence = np.mean(list(confidences.values()))
        
        if consensus and avg_confidence > 0.8:
            return "LOW"
        elif consensus and avg_confidence > 0.6:
            return "MEDIUM"
        elif not consensus and avg_confidence > 0.7:
            return "MEDIUM"
        else:
            return "HIGH"
    
    def _generate_recommendation(self, prediction, confidence, risk_level):
        """Generate trading recommendation"""
        
        if prediction == 1 and confidence > 0.75 and risk_level in ["LOW", "MEDIUM"]:
            return "STRONG BUY"
        elif prediction == 1 and confidence > 0.6:
            return "BUY"
        elif prediction == 1 and confidence > 0.5:
            return "WEAK BUY"
        elif prediction == 0 and confidence > 0.75:
            return "AVOID"
        else:
            return "NEUTRAL"
    
    # Helper methods for advanced feature extraction
    def _calculate_skewness(self, data):
        from scipy.stats import skew
        return skew(data)
    
    def _calculate_kurtosis(self, data):
        from scipy.stats import kurtosis
        return kurtosis(data)
    
    def _calculate_autocorr(self, data, lag):
        if len(data) <= lag:
            return 0
        return np.corrcoef(data[:-lag], data[lag:])[0, 1] if not np.isnan(np.corrcoef(data[:-lag], data[lag:])[0, 1]) else 0
    
    # ... other helper methods would be implemented here
    
    def save_models(self, filepath):
        """Save trained models to disk"""
        import joblib
        joblib.dump(self.models, filepath)
        print(f"Models saved to {filepath}")
    
    def load_models(self, filepath):
        """Load trained models from disk"""
        import joblib
        self.models = joblib.load(filepath)
        print(f"Models loaded from {filepath}")

# Usage example
# production_system = ProductionVPAMLSystem()
# models = production_system.train_ensemble_model(X_train, y_train, X_val, y_val)
# prediction_result = production_system.real_time_prediction_pipeline(current_stock_data)
```

</details>

---

## ğŸ“‹ TÃ³m Táº¯t ChÆ°Æ¡ng

### Nhá»¯ng GÃ¬ ÄÃ£ Há»c:
1. **Chuáº©n bá»‹ dá»¯ liá»‡u cho AI** - Features, labels, preprocessing
2. **Huáº¥n luyá»‡n AI model** - Random Forest Ä‘á»ƒ nháº­n diá»‡n VPA
3. **AI Scanner tá»± Ä‘á»™ng** - QuÃ©t toÃ n thá»‹ trÆ°á»ng tÃ¬m cÆ¡ há»™i
4. **Backtesting AI** - Kiá»ƒm tra hiá»‡u quáº£ model
5. **Deep Learning CNN** - Nháº­n diá»‡n patterns tá»« chart images (nÃ¢ng cao)
6. **Production System** - Há»‡ thá»‘ng ML hoÃ n chá»‰nh (nÃ¢ng cao)

### Lá»£i Ãch Thiáº¿t Thá»±c:
- âœ… **Tiáº¿t kiá»‡m thá»i gian** - AI quÃ©t 1000 cá»• phiáº¿u trong vÃ i giÃ¢y
- âœ… **KhÃ´ng bá» sÃ³t cÆ¡ há»™i** - MÃ¡y tÃ­nh khÃ´ng má»‡t má»i, khÃ´ng cáº£m xÃºc
- âœ… **KhÃ¡ch quan** - Quyáº¿t Ä‘á»‹nh dá»±a trÃªn dá»¯ liá»‡u, khÃ´ng subjective
- âœ… **Scalable** - CÃ³ thá»ƒ má»Ÿ rá»™ng cho toÃ n bá»™ thá»‹ trÆ°á»ng VN
- âœ… **24/7 Monitoring** - Theo dÃµi liÃªn tá»¥c, cáº£nh bÃ¡o realtime

### Cáº£nh BÃ¡o Quan Trá»ng:
> âš ï¸ **AI chá»‰ lÃ  cÃ´ng cá»¥ há»— trá»£, khÃ´ng pháº£i thay tháº¿ cho kiáº¿n thá»©c VPA**
> - AI cÃ³ thá»ƒ nháº§m trong nhá»¯ng tÃ¬nh huá»‘ng báº¥t thÆ°á»ng
> - LuÃ´n káº¿t há»£p AI prediction vá»›i phÃ¢n tÃ­ch thá»§ cÃ´ng
> - Backtest tá»‘t khÃ´ng Ä‘áº£m báº£o tÆ°Æ¡ng lai sáº½ tá»‘t

### ChÆ°Æ¡ng Tiáº¿p Theo:
**ChÆ°Æ¡ng 5.5: PhÃ¢n TÃ­ch Äa Thá»‹ TrÆ°á»ng** - Káº¿t há»£p VPA Viá»‡t Nam vá»›i thá»‹ trÆ°á»ng toÃ n cáº§u Ä‘á»ƒ Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh tá»‘t hÆ¡n.